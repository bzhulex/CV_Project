<!---
Authors: Garrison Ramey,
Date: 2020-10-27
Time: 3:19 PM EDT
--->
<!DOCTYPE html>
<html lang="">
  <head>
    <meta charset="utf-8">
    <title>Final Project</title>
    <link rel="stylesheet" type="text/css" href="./css/style.css" />
    <!--- MathJax --->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
    </script>
  </head>
  <body>
    
    <center>
    <h1>Video Shot Boundary Detection and Keyframe Extraction</h1>
    <h3>Garrison Ramey, Alex Karwoski, Brian Zhu, Taylan Selman</h3>
    <h3> Fall 2020 </h3>
    </center>
    <br>
    <hr class="heavy">
    <br>
    <!--- Abstract --->
    <h1>Abstract</h1>
      <p>
        As the amount of video content available and consumed on social media platforms has grown 
        exponentially in the past decade, so has the necessity for classifying and summarizing videos with 
        high accuracy. The goal of this project is to survey and analyze two state-of-the-art keyframe 
        extraction techniques using multiple public video datasets and compare their performances. We will 
        use the keyframe annotations in the dataset as the ground truth keyframes in order to compute 
        the accuracy, recall, precision, and figure of merit (F-score) score for each method. We expect 
        our experiments to show successful keyframe selection by producing good quantitative results 
        and visual representations that accurately summarize each video.
      </p>
    <br>
    <hr class="heavy">
    <br>
    <!--- Teaser Figure --->
    <h1> Teaser Figure </h1>
      <p> 
        <center>
          <img src="./images/teaser-figure.png" style="width: 75%; height: 75%; border: 5px solid;">
        </center>
      </p>
    <br>
    <hr class="heavy">
    <br>
    <!--- Introduction --->
    <h1> Introduction </h1>
      <p>
        A key tool for real-time video processing is keyframe extraction. Keyframe extraction seeks to 
        search for frames that best represent the content of each shot or best represent the scene in the 
        video clip. The main processing of keyframe extraction is shot segmentation. Shot segmentation 
        involves detecting the transition between successive shots.
      </p>
      <p>
        This transition can be represented numerically by comparing different features from consecutive frames. These features vary between different keyframe extraction algorithms and pinpoint different similarities to accurately determine the keyframes for a video.
      </p>
      <p>
        We will implement a detection-based keyframe extraction algorithm based on a combination of two 
        features: the Pearson correlation coefficient (PCC) and color moments (CM). The second method we 
        will be using is non-sequential called Edge Aware Clustering, meaning that instead of directly 
        comparing consecutive frames to detect a keyframe, the frames will first be clustered into different 
        shots (a collection of frames). We will test both of these approaches on color image frames with 
        differing actions to determine the more accurate methods. 
      </p>
    <br>
    <hr class="heavy">
    <br>
    <!--- Approach --->
    <h1> Approach </h1>
    <h3> Method 1: PCC and Color Moment</h3>
      <p>
        The first approach we used exploited Pearson correlation coefficients (PCC) and color moments 
        (CM) based on the research by Bommisetty et al. [2]. The linear transformation invariance property 
        of PCC facilitates the proposed algorithm to work well under varying lighting conditions. On the 
        other hand, the scale and rotation invariance properties of color moments are beneficial for 
        representation of complex objects that may be present in different poses and orientations.
      </p>
      <p>
        In order to implement this approach, we first considered how we should use PCC to find shot boundaries. 
        According to Bommisetty et al. [2], each color channel needed to be analyzed independently for the 
        sequence of frames. Therefore, arrays were created for each color channel to store the PCC between 
        all consecutive frames, and the PCC between two consecutive frames was 
        computed according to the equation below.
      </p>
      <center>
      <p>
        $ \Gamma = \frac {\mid cov(x, y) \mid} {{\sigma_{x}}{\sigma_{y}}} $
      </p>
      </center>
      <p>
        With the PCC arrays, we then computed the threshold value, denoted as $T_{c}$, for each channel in 
        the following equation where $\alpha$ is a hyperparameter, $\sigma_{c}$ is the variance for the color 
        channel, and $c$ is a red, green, or blue channel.
      </p>
      <p>
        <center>
          $ T_{c} = \mu_{c} + (\alpha * \sigma_{c}^{2}) $
        </center>
      </p>
      <p>
        In total, there were three total thresholds, one for each of the three PCC arrays. Using the 
        thresholds, we found the set of specific video frames that would be used to determine the
        final set of shot boundaries.
      </p>
      <p>
        To find another set of potential shot boudaries, the second feature set of the video frames 
        we utilized is color moments. A color moment for a single frame is composed of four features: 
        mean, standard deviation, skewness, and kurtosis.
      </p>
      <p>
        The mean ($m$), standard deviation ($\sigma$), skewness ($S$) and kurtosis ($K$) were computed by the 
        following equations where $N$ is the total number of frames, $M$ is the number of grayscale levels for
        a particular frame, $G_{j}$ is the $jth$ gray level, and $C_{j}$ is the pixel count in 
        grayscale-level $j$.
      </p>
      <center>
        <p>$ m = \frac{1}{N} \sum\limits_{j = 1}^{M} G_{j} C_{j} $</p>
        <p>$ \sigma = \sqrt{\frac{1}{(N - 1)} \sum\limits_{j = 1}^{M}(G_{j} - m)^{2}C_{j}} $</p>
        <p>$ S = \frac{1}{(N - 1)\sigma^{3}} \sum\limits_{j = 1}^{M} (G_{j} - m)^{3}C_{j} $</p>
        <p>$ K = \frac{1}{(N - 1)\sigma^{4}} \sum\limits_{j = 1}^{M} (G_{j} - m)^{4}C_{j} $</p>
      </center>
      <p>
        For each frame, we stored the value of the color moment features in four separate arrays. Then, the 
        mean difference arrays for each color moment feature were found, denoted
        as $D_{m}$, $D_{\sigma}$, $D_{S}$, $D_{K}$, and the thresholds were computed as follows.
      </p>
      <center>
          <p>$ T_{D_{m}} = \alpha * \mu_{D_{m}}$</p>
          <p>$ T_{D_{\sigma}} = \alpha * \mu_{D_{\sigma}}$</p>
          <p>$ T_{D_{S}} = \alpha * \mu_{D_{S}}$</p>
          <p>$ T_{D_{K}} = \alpha * \mu_{D_{K}}$</p>
      </center>
      <p>
        Using each difference threshold, we found the second set of frames that would be potential shot boundaries.
      </p>
      <p>
        We found our final shot boundaries by taking the intersection of the set of boundaries found 
        using PCC and the set found using color moments. Then, for each of the final shot boundaries, we 
        extracted the frame with the highest color moment mean and standard deviation, and all the frames 
        chosen were declared as keyframes for the video.
      </p>
      <h3>Method 2: Edge Aware Clustering</h3>
      <p> 
          Our second approach, based on the research by Priya and Domnic [3], uses feature extraction, block categorization, and 
          continuity values to cluster and select keyframes. Instead of considering the entire frame as done in method 1, each frame 
          was split into 4 x 4 pixel blocks, and each block was categorized as either an edge block or a non-edge block according 
          to specific criteria. Based on the similarity between all corresponding pixel blocks among consecutive frames,
          a continuity value was determined. After clustering the continuity values for all frames, we used an extraction technique that 
          accounted for cluster sizes as well as visual content variations within a shot.
      </p>
      <p>To start, we first found the edge pattern ($EP$) of each block according to the equations below, where $Thr$ is equal to 16, 
        $m$ is the median intensity of the block, and $\delta$ is the maximum intensity value of the block.
      </p>
      <p class="equation"> $ \varepsilon = \delta - m $ </p>
      <p class="equation"> $ EP = \begin{cases} \mbox{1} & \mbox{if } (\varepsilon > Thr \text{&} x_{p} > \bar{x}) \\ \mbox{0} & \mbox{otherwise} \end{cases}$ </p>
      <p>Once the edge patterns for each block were found, we had to classify each one as either an edge or non-edge block. According to 
        Priya and Domnic, a block is an edge block if the number of edge pixels in the edge pattern exceeds
        half of the total block size; otherwise, the block is declared as a non-edge block. Once all blocks were classified, we computed the
        similarity scores between corresponding blocks of consecutive frames.
      </p>

      <p>
        In order to compute the similarity scores, we considered two quantized values to represent a single block:
        a lower mean ($\bar{\alpha}$) and a higher mean ($\bar{\beta}$). The quantized values were computed as follows
        where $\bar{x}$ is the mean intensity value of the block, $q$ is the number of pixels in the block greater than $\bar{x}$,
        and $b(x)$ is all of the intensity values of the block.
      </p>
      <p class="equation">$ \bar{\alpha} = \frac {1} {16 - q} \sum\limits_{b(x) < \bar{x}} b(x) $</p>
      <p class="equation">$ \bar{\beta} = \frac {1} {q} \sum\limits_{b(x) \geq \bar{x}} b(x) $</p> 

      <p>
        Next, we compared corresponding blocks of consecutive frames using $a$, $b$, and $x$ to represent edge blocks and $\bar{x}$ 
        and $q'$ to represent non-edge blocks, where $q'$ is the number of edge pixels in the edge pattern. If the corresponding blocks
        were both edge blocks or both non-edge blocks, then we used the following equations to compute the similarity between them,
        where the difference value of the $jth$ block is denoted as $D_{j}$.
      </p>
      <p class="equation"> 
        $ D_{j} = \lvert q_{i, j} - q_{i+1, j} \rvert 
        (\lvert \bar{\alpha}_{i, j} - \bar{\alpha}_{i + 1, j} \rvert + \lvert \bar{\beta}_{i, j} - \bar{\beta}_{i+1, j}\rvert)$
      </p>
      <p class="equation">
         $ D_{j} = 2 \lvert q'_{i, j} - q'_{i+1, j} \rvert \lvert \bar{x}_{i, j} - \bar{x}_{i+1, j} \rvert $
      </p>
      <p>
        If corresponding blocks between the frame $i$ and $i + 1$ were edge and non-edge blocks, or non-edge and edge blocks 
        respectively, then the difference value was computed as follows.
      </p>
      <p class="equation">
        $ D_{j} = \lvert q_{i, j} - q'_{i + 1, j} \rvert \lvert \bar{\alpha}_{i, j} + \bar{\beta}_{i, j} - 2\bar{x}_{i + 1, j} \rvert $
      </p>
      <p class="equation">
        $ D_{j} = \lvert q'_{i, j} - q_{i + 1, j} \rvert \lvert 2 \bar{x}_{i, j} - (\bar{\alpha}_{i + 1, j} + \bar{\beta}_{i+1, j}) \rvert $
      </p>
      <p>
       Then, we defined a continuity value ($\lambda$) for all consecutive frames as $\lambda = \sum\limits_{j=1}^{n} D_{j} $
      </p>
      <p>Using the continuity values for the full sequence of frames, we clustered the frames into shots using the method below. </p>
      <center>
        <img src="./images/alg1-m2.png" style="width:50%; height:50%; border: 5px solid" >
        </center>
      <p> Once all shots were constructed, we extracted keyframes from each shot using the keyframe extraction method below</p>
      <center>
        <img src="./images/alg2-m2.png" style="width:55%; height:55%; border: 5px solid;" >
        </center>
    <br>
    <hr class="heavy">
    <br>
    <!--- Experiments and Results --->
    <h1>Experiments and Results </h1>
      <center>
        <div>
          <figure>
            <img src="images/results.png" style="width: 75%; height: 75%; border: 5px solid;">
            <figcaption>PCC and Color Moment Results</figcaption>
          </figure>
        </div>

        <div>
          <figure>
            <img src="images/results_edge.png" style="width: 75%; height: 75%; border: 5px solid;">
            <figcaption>Edge-Aware Clustering Results</figcaption>
          </figure>
        </div>
      </center>
      <p>
       We tested both of our keyframe extraction techniques on two different videos. The first video depicts a monkey climbing on a rock (Video 3) and the second video shows the view of a city from a distance (Video 17). The dataset we used contained a set of ground-truth keyframes, to which we compared our results from the extraction method using four key metrics: accuracy, precision, recall, and F-score. We defined accuracy as the sum of the true positives and true negatives over the sum of the true positives, true negatives, false positives, and false negatives. 
      </p>
      <p>
        For the first technique, the PCC and Color Moment algorithm, we found our method to have relatively low accuracy for both videos, locating the correct keyframes under 50% of the time. Next, we defined precision as the number of true positives over the sum of the false and true positives. Our methods were less precise than accurate, with frame precision as low as 25% for the second video. We defined recall as the number of true positives over the sum of true positives and false negatives. Recall was the highest performing metric, indicating that our method did well at identifying the ground-truth key frames, if not accounting for false positives. We also evaluated performance based on “F-score", which is equal to 2 times precision times recall over the sum of precision and recall.
      </p>
      <p>
        For the second method, Edge-Aware Clustering the results were just slightly more accurate on average. The accuracy of this algorithm was 42% for the first video and 48% for the second video, still under 50% for both. On average this method was less precise than the first with values of 33.1% and 25.5% respectively. Unlike the first method, the recall of the Edge-Aware algorithm was not the best performing metric with an average of 38% between the two videos. Finally the F-Score of this method was also lower than the first, with a value of 31.9% for the first video and 32.5% for the second video. 
      </p>
    <br>
    <hr class="heavy">
    <br>
    <!--- Qualitative Results --->
    <h1> Qualitative Results </h1>
    <center>
      <div>
      <h4>Original Video</h4>
      <video width="400" height="230" controls>
        <source src="videos/Video03_Monkey.mp4">
      </video>
      </div>
      <div>
        <h4>Keyframes Selected</h4>
        <img class="reduced" src="images/vid-3-figure.png">
      </div>
      <h4>Original Video</h4>
      <video width="400" height="230" controls>
        <source src="videos/Video17_Stockholm.mp4">
      </video>
      </div>
      <div>
        <h4>Keyframes Selected</h4>
        <img class="reduced" src="images/vid-17-figure.png">
      </div>
    </center>
    <br>
    <hr class="heavy">
    <br>
    <!--- Conclusion and Future Work --->
    <h1> Conclusion and Future Work </h1>
      <p>
        As we discussed earlier, it is necessary to classify and summarize videos with high accuracy, 
        as the amount of video content available and consumed on social media platforms has grown 
        exponentially in the past decade. Our results with both PPC and Color Moment as well as Edge-Aware Clustering show fair accuracy, averaging 44.3% accuracy across both videos. While Edge-Aware Clustering showed a slight improvement in average accuracy, Bommisetty et al. propose that PCC and Color Moment should be the better performer due to the compounding benifits from linear transformation variance. We theorize that this is due to testing on a dataset consiting of 2 videos, if a greater number of videos were tested the improved accuracy of PCC should reveal itself. In both methods, we used the keyframe annotations from the dataset as ground truth keyframes in order to computer the accuracy, recall, precision, and figure of merit (F-score) score for each algorithm. Possible routes of improvement might be to expand our datasets to longer videos to examine any differences in performance. Another idea could be to run our methods on a large selection of smaller videos and run clustering algorithms on the extracted frames to
        attempt classification.
      </p>
    <br>
    <hr class="heavy">
    <br>
    <!--- References --->
    <h1> References </h1>
      <p>[1] S. Pandey, P. Dwivedy, S. Meena and A. Potnis, “A survey on key frame extraction methods of a MPEG video,” 2017 International Conference on Computing, Communication and Automation (ICCCA), Greater Noida, 2017, pp. 1192-1196, doi: 10.1109/CCAA.2017.8229979.</p>
      <p>[2] Bommisetty, R. M., Prakash, O., & Khare, A. (2019). Keyframe extraction using Pearson correlation coefficient and color moments. Multimedia Systems, 1-33.</p>
      <p>[3] G.L. Priya, S. Domnic Shot based keyframe extraction for ecological video indexing and retrieval Ecological Inform., 23 (2014), pp. 107-117.</p>
  </body>
</html>
